{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2ecd0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(0.0, 1.0, (11,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class DDoSEnv(gym.Env):\n",
    "    def __init__(self, dataset):\n",
    "        super(DDoSEnv, self).__init__()\n",
    "        self.data = dataset\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(self.data) - 1\n",
    "        \n",
    "        # Define action and observation space\n",
    "        self.action_space = gym.spaces.Discrete(2)  # Normal or DDoS\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(11,), dtype=np.float32)  \n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        obs = pd.get_dummies(self.data.iloc[self.current_step][['Source IP']])\n",
    "        obs = obs.values.astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        reward = self._get_reward(action)\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def _get_reward(self, action):\n",
    "        label = self.data.iloc[self.current_step]['Label']  # Assuming the column name is DDoS_Label\n",
    "        if action == label:\n",
    "            return 1.0  # Correct detection\n",
    "        else:\n",
    "            return -1.0  # Incorrect detection\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = ('/Users/selmael-korchi/db2.csv' )\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "# Remove leading spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "# Create environment\n",
    "env = DDoSEnv(dataset)\n",
    "\n",
    "# Verify environment setup\n",
    "observation = env.reset()\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac3c5306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 104, Exploration Rate = 0.99\n",
      "Episode 2: Total Reward = 198, Exploration Rate = 0.9801\n",
      "Episode 3: Total Reward = -38, Exploration Rate = 0.9702989999999999\n",
      "Episode 4: Total Reward = 242, Exploration Rate = 0.96059601\n",
      "Episode 5: Total Reward = 312, Exploration Rate = 0.9509900498999999\n",
      "Episode 6: Total Reward = 202, Exploration Rate = 0.9414801494009999\n",
      "Episode 7: Total Reward = 330, Exploration Rate = 0.9320653479069899\n",
      "Episode 8: Total Reward = 560, Exploration Rate = 0.92274469442792\n",
      "Episode 9: Total Reward = 614, Exploration Rate = 0.9135172474836407\n",
      "Episode 10: Total Reward = 708, Exploration Rate = 0.9043820750088043\n",
      "Episode 11: Total Reward = 734, Exploration Rate = 0.8953382542587163\n",
      "Episode 12: Total Reward = 960, Exploration Rate = 0.8863848717161291\n",
      "Episode 13: Total Reward = 956, Exploration Rate = 0.8775210229989678\n",
      "Episode 14: Total Reward = 1032, Exploration Rate = 0.8687458127689781\n",
      "Episode 15: Total Reward = 1138, Exploration Rate = 0.8600583546412883\n",
      "Episode 16: Total Reward = 1310, Exploration Rate = 0.8514577710948754\n",
      "Episode 17: Total Reward = 1318, Exploration Rate = 0.8429431933839266\n",
      "Episode 18: Total Reward = 1322, Exploration Rate = 0.8345137614500874\n",
      "Episode 19: Total Reward = 1392, Exploration Rate = 0.8261686238355865\n",
      "Episode 20: Total Reward = 1418, Exploration Rate = 0.8179069375972307\n",
      "Episode 21: Total Reward = 1462, Exploration Rate = 0.8097278682212583\n",
      "Episode 22: Total Reward = 1512, Exploration Rate = 0.8016305895390458\n",
      "Episode 23: Total Reward = 1846, Exploration Rate = 0.7936142836436553\n",
      "Episode 24: Total Reward = 1728, Exploration Rate = 0.7856781408072188\n",
      "Episode 25: Total Reward = 1728, Exploration Rate = 0.7778213593991465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/21/nly_w3c50pd6j2qh3ypgb5cm0000gn/T/ipykernel_807/2200262874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Take action, observe next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Convert next_state to integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/21/nly_w3c50pd6j2qh3ypgb5cm0000gn/T/ipykernel_807/2200262874.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/21/nly_w3c50pd6j2qh3ypgb5cm0000gn/T/ipykernel_807/2200262874.py\u001b[0m in \u001b[0;36m_next_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Source IP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_first\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m         )\n\u001b[1;32m    910\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;31m# Series avoids inconsistent NaN handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m     \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfactorize_from_iterable\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2695\u001b[0m         \u001b[0;31m# but only the resulting categories, the order of which is independent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m         \u001b[0;31m# from ordered. Set ordered to False as default. See GH #15457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         codes, uniques = _factorize_array(\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         )\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_factorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     uniques, codes = table.factorize(\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class DDoSEnv(gym.Env):\n",
    "    def __init__(self, dataset):\n",
    "        super(DDoSEnv, self).__init__()\n",
    "        self.data = dataset\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(self.data) - 1\n",
    "        \n",
    "        # Define action and observation space\n",
    "        self.action_space = gym.spaces.Discrete(2)  # Normal or DDoS\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(11,), dtype=np.float32)  \n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        obs = pd.get_dummies(self.data.iloc[self.current_step][['Source IP']])\n",
    "        obs = obs.values.astype(np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        correct_action = self.data.iloc[self.current_step]['Label']\n",
    "        correct_action_value = 1 if correct_action == \"Portmap\" else 0\n",
    "        reward = 1 if action == correct_action_value else -1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        next_state = self._next_observation() if not done else None\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = ('/Users/selmael-korchi/db2.csv')\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset.columns = dataset.columns.str.strip()  # Remove leading spaces from column names\n",
    "\n",
    "# Create environment\n",
    "env = DDoSEnv(dataset)\n",
    "\n",
    "# Define Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.6  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate (start with exploration)\n",
    "\n",
    "# Initialize Q-table\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros([num_states, num_actions])\n",
    "\n",
    "# Define training parameters\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = len(dataset)\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Convert state to integer index\n",
    "        state_index = np.argmax(state)\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state_index, :])  # Exploit learned values\n",
    "        \n",
    "        # Take action, observe next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Convert next_state to integer index\n",
    "        next_state_index = np.argmax(next_state)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_table[state_index, action] = (1 - alpha) * q_table[state_index, action] + alpha * (reward + gamma * np.max(q_table[next_state_index, :]))\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(0.1, epsilon * 0.99)  # Ensure epsilon doesn't go below 0.1\n",
    "    \n",
    "    # Print episode statistics\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Exploration Rate = {epsilon}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3391c10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 2: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 3: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 4: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 5: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 6: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 7: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 8: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 9: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 10: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 11: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 12: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 13: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 14: Total Reward = -9998.0, Exploration Rate = 0.1\n",
      "Episode 15: Total Reward = -9998.0, Exploration Rate = 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/21/nly_w3c50pd6j2qh3ypgb5cm0000gn/T/ipykernel_807/3263501655.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Take action, observe next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/21/nly_w3c50pd6j2qh3ypgb5cm0000gn/T/ipykernel_807/3263501655.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/21/nly_w3c50pd6j2qh3ypgb5cm0000gn/T/ipykernel_807/3263501655.py\u001b[0m in \u001b[0;36m_next_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Source IP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   2827\u001b[0m         \u001b[0;31m# irow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_interleaved_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_interleaved_dtype\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1893\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1895\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfind_common_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mfind_common_type\u001b[0;34m(types)\u001b[0m\n\u001b[1;32m   1498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_common_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36mfind_common_type\u001b[0;34m(array_types, scalar_types)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \"\"\"\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0marray_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m     \u001b[0mscalar_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscalar_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \"\"\"\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0marray_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m     \u001b[0mscalar_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscalar_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class DDoSEnv(gym.Env):\n",
    "    def __init__(self, dataset):\n",
    "        super(DDoSEnv, self).__init__()\n",
    "        self.data = dataset\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(self.data) - 1\n",
    "        \n",
    "        # Define action and observation space\n",
    "        self.action_space = gym.spaces.Discrete(2)  # Normal or DDoS\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(11,), dtype=np.float32)  \n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        obs = pd.get_dummies(self.data.iloc[self.current_step][['Source IP']])\n",
    "        obs = obs.values.astype(np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        reward = self._get_reward(action)\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def _get_reward(self, action):\n",
    "        label = self.data.iloc[self.current_step]['Label']  # Assuming the column name is DDoS_Label\n",
    "        if action == label:\n",
    "            return 1.0  # Correct detection\n",
    "        else:\n",
    "            return -1.0  # Incorrect detection\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = ('/Users/selmael-korchi/db2.csv' )\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "# Remove leading spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Create environment\n",
    "env = DDoSEnv(dataset)\n",
    "\n",
    "# Define Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.6  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Initialize Q-table\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros([num_states, num_actions])\n",
    "\n",
    "# Define training parameters\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = len(dataset)\n",
    "\n",
    "# Q-learning algorithm\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "\n",
    "        # Convert state to integer index\n",
    "        state_index = np.argmax(state)\n",
    "\n",
    "        # Exploration-exploitation trade-off\n",
    "\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state_index, :])  # Exploit learned values\n",
    "\n",
    "        # Take action, observe next state and reward\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "        # Convert next_state to integer index\n",
    "        next_state_index = np.argmax(next_state)\n",
    "        \n",
    "\n",
    "        # Update Q-table\n",
    "\n",
    "        q_table[state_index, action] = (1 - alpha) * q_table[state_index, action] + alpha * (reward + gamma * np.max(q_table[next_state_index, :]))\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(0.1, epsilon * 0.99)\n",
    "    # Print episode statistics\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Exploration Rate = {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9109fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([' Source IP', ' Source Port', ' Destination IP', ' Destination Port',\n",
      "       ' Timestamp', ' Total Fwd Packets', ' Total Backward Packets'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/Users/selmael-korchi/Dataset-DDoS.csv\"\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Verify column names\n",
    "print(dataset.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd176b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Source IP   Source Port  Destination IP   Destination Port  \\\n",
      "0     192.168.50.254             0       224.0.0.5                  0   \n",
      "1     192.168.50.253             0       224.0.0.5                  0   \n",
      "2       192.168.50.6         54799   172.217.10.98                443   \n",
      "3       192.168.50.6         54800     172.217.7.2                443   \n",
      "4       192.168.50.6         54801   172.217.10.98                443   \n",
      "...              ...           ...             ...                ...   \n",
      "9994      172.16.0.5           648    192.168.50.4              38265   \n",
      "9995      172.16.0.5           648    192.168.50.4              20343   \n",
      "9996      172.16.0.5           642    192.168.50.4              48659   \n",
      "9997      172.16.0.5           648    192.168.50.4              14536   \n",
      "9998      172.16.0.5           648    192.168.50.4              46858   \n",
      "\n",
      "                       Timestamp   Total Fwd Packets   Total Backward Packets  \n",
      "0     2018-11-03 09:18:16.964447                  45                        0  \n",
      "1     2018-11-03 09:18:18.506537                  56                        0  \n",
      "2     2018-11-03 09:18:18.610576                   6                        2  \n",
      "3     2018-11-03 09:18:18.610579                   6                        2  \n",
      "4     2018-11-03 09:18:18.610581                   6                        2  \n",
      "...                          ...                 ...                      ...  \n",
      "9994  2018-11-03 10:01:26.378691                   2                        0  \n",
      "9995  2018-11-03 10:01:26.378693                   2                        0  \n",
      "9996  2018-11-03 10:01:26.378743                   2                        0  \n",
      "9997  2018-11-03 10:01:26.378795                   2                        0  \n",
      "9998  2018-11-03 10:01:26.378797                   2                        0  \n",
      "\n",
      "[9999 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from CSV\n",
    "dataset_path = \"/Users/selmael-korchi/Dataset-DDoS.csv\"\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Specify the path where you want to save the Excel file\n",
    "excel_path = \"/Users/selmael-korchi/Dataset-DDoS-new.xlsx\"\n",
    "\n",
    "# Convert the dataset to Excel format and save it\n",
    "dataset.to_excel(excel_path, index=False)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9f799bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class DDoSEnv(gym.Env):\n",
    "    def __init__(self, dataset_path):\n",
    "        super(DDoSEnv, self).__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.load_dataset()\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        self.dataset = pd.read_csv(self.dataset_path)\n",
    "    def reset(self):\n",
    "        # Reset environment state\n",
    "        pass\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Perform one step in the environment\n",
    "        pass\n",
    "    \n",
    "    def render(self):\n",
    "        # Render environment state\n",
    "        pass\n",
    "\n",
    "# Example usage:\n",
    "dataset_path = \"/Users/selmael-korchi/Dataset-DDoS.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b581b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: Ellipsis\n",
      "Reward: Ellipsis\n",
      "Done: Ellipsis\n",
      "Info: {}\n",
      "Episode finished after 1 steps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "class DDoSEnv(gym.Env):\n",
    "    def __init__(self, dataset):\n",
    "        super(DDoSEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(2)  \n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        observation = ...  # Compute the observation after taking the action\n",
    "        reward = ...  # Compute the reward based on the action and new state\n",
    "        done = ...  # Determine if the episode is finished\n",
    "        info = {}  # Additional information, if needed\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "\n",
    "\n",
    "dataset_path = \"/Users/selmael-korchi/Dataset-DDoS.csv\"  # Replace with your dataset path\n",
    "env = DDoSEnv(dataset_path)\n",
    "\n",
    "# Reset the environment to its initial state\n",
    "observation = env.reset()\n",
    "\n",
    "# Perform some steps in the environment and observe the results\n",
    "for _ in range(10):\n",
    "    # Sample a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Perform the action and observe the next state, reward, and whether the episode is done\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Observation:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    \n",
    "    # Check if the episode is done\n",
    "    if done:\n",
    "        print(\"Episode finished after\", _ + 1, \"steps\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd059e64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [0.55648471 0.43039136 0.48305208 0.92300214 0.44471455 0.48877233\n",
      " 0.94000009 0.25164366 0.42336469 0.99787281] Reward: 5.939298431718971 Done: False\n",
      "Observation: [0.55648471 0.43039136 0.48305208 0.92300214 0.44471455 0.48877233\n",
      " 0.94000009 0.25164366 0.42336469 0.99787281] Reward: 5.939298431718971 Done: False\n",
      "Observation: [1.55648471 1.43039136 1.48305208 1.92300214 1.44471455 1.48877233\n",
      " 1.94000009 1.25164366 1.42336469 1.99787281] Reward: 5.939298431718971 Done: False\n",
      "Observation: [2.55648471 2.43039136 2.48305208 2.92300214 2.44471455 2.48877233\n",
      " 2.94000009 2.25164366 2.42336469 2.99787281] Reward: 15.939298431718973 Done: False\n",
      "Observation: [3.55648471 3.43039136 3.48305208 3.92300214 3.44471455 3.48877233\n",
      " 3.94000009 3.25164366 3.42336469 3.99787281] Reward: 25.93929843171897 Done: False\n",
      "Observation: [4.55648471 4.43039136 4.48305208 4.92300214 4.44471455 4.48877233\n",
      " 4.94000009 4.25164366 4.42336469 4.99787281] Reward: 35.93929843171897 Done: False\n",
      "Observation: [5.55648471 5.43039136 5.48305208 5.92300214 5.44471455 5.48877233\n",
      " 5.94000009 5.25164366 5.42336469 5.99787281] Reward: 45.93929843171897 Done: False\n",
      "Observation: [5.55648471 5.43039136 5.48305208 5.92300214 5.44471455 5.48877233\n",
      " 5.94000009 5.25164366 5.42336469 5.99787281] Reward: 55.93929843171897 Done: False\n",
      "Observation: [5.55648471 5.43039136 5.48305208 5.92300214 5.44471455 5.48877233\n",
      " 5.94000009 5.25164366 5.42336469 5.99787281] Reward: 55.93929843171897 Done: False\n",
      "Observation: [6.55648471 6.43039136 6.48305208 6.92300214 6.44471455 6.48877233\n",
      " 6.94000009 6.25164366 6.42336469 6.99787281] Reward: 55.93929843171897 Done: False\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class DDoSEnv(gym.Env):\n",
    "    def __init__(self, observation_size, num_actions, max_steps):\n",
    "        # Define observation space\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(observation_size,), dtype=np.float32)\n",
    "        \n",
    "        # Define action space\n",
    "        self.action_space = gym.spaces.Discrete(num_actions)\n",
    "        \n",
    "        # Define other attributes as needed\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.observation = np.random.rand(observation_size)  # Initialize with random observation\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state\n",
    "        self.current_step = 0\n",
    "        self.observation = np.random.rand(self.observation_space.shape[0])  # Reset to random observation\n",
    "        \n",
    "        return self.observation\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Update the environment based on the action\n",
    "        # Calculate the next observation, reward, and whether the episode is done\n",
    "        \n",
    "        # Example:\n",
    "        next_observation = self.observation + action  # Replace with actual environment dynamics\n",
    "        reward = self._calculate_reward()  # Implement reward calculation\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        self.observation = next_observation\n",
    "        \n",
    "        return next_observation, reward, done, {}\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        # Implement reward calculation based on the current observation\n",
    "        # Example:\n",
    "        return np.sum(self.observation)\n",
    "\n",
    "# Example usage:\n",
    "observation_size = 10\n",
    "num_actions = 2\n",
    "max_steps = 100\n",
    "\n",
    "env = DDoSEnv(observation_size, num_actions, max_steps)\n",
    "observation = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # Sample a random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(\"Observation:\", observation, \"Reward:\", reward, \"Done:\", done)\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb454106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "file_path = \"/Users/selmael-korchi/Dataset-DDoS-new.xlsx\"\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Remove leading spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Manually label instances based on criteria\n",
    "# For example, let's say instances with more than 100 total forward packets are labeled as attacks\n",
    "df['label'] = df['Total Fwd Packets'].apply(lambda x: 1 if x > 100 else 0)\n",
    "\n",
    "# Save the labeled dataset\n",
    "labeled_file_path = \"/Users/selmael-korchi/Labeled-Dataset-DDoS.xlsx\"\n",
    "df.to_excel(labeled_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52bf730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -141, Correct Actions = 3929, Exploration Rate = 0.99\n",
      "Episode 2: Total Reward = 283, Correct Actions = 4141, Exploration Rate = 0.9801\n",
      "Episode 3: Total Reward = 279, Correct Actions = 4139, Exploration Rate = 0.9702989999999999\n",
      "Episode 4: Total Reward = 253, Correct Actions = 4126, Exploration Rate = 0.96059601\n",
      "Episode 5: Total Reward = 189, Correct Actions = 4094, Exploration Rate = 0.9509900498999999\n",
      "Episode 6: Total Reward = 451, Correct Actions = 4225, Exploration Rate = 0.9414801494009999\n",
      "Episode 7: Total Reward = 415, Correct Actions = 4207, Exploration Rate = 0.9320653479069899\n",
      "Episode 8: Total Reward = 637, Correct Actions = 4318, Exploration Rate = 0.92274469442792\n",
      "Episode 9: Total Reward = 687, Correct Actions = 4343, Exploration Rate = 0.9135172474836407\n",
      "Episode 10: Total Reward = 619, Correct Actions = 4309, Exploration Rate = 0.9043820750088043\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load and prepare the data\n",
    "\n",
    "db_path = '/Users/selmael-korchi/db2.csv'  # Adjust the file path\n",
    "data = pd.read_csv(db_path)\n",
    "\n",
    "# Assuming there's a 'Label' column indicating 1 for DDOS attack and 0 for normal\n",
    "# Simplified data preprocessing\n",
    "features = ['Source Port', 'Destination Port', 'Total Fwd Packets', 'Total Backward Packets']  \n",
    "X = data[features]\n",
    "y = data['Label']  \n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train).reset_index(drop=True)\n",
    "X_test = pd.DataFrame(X_test).reset_index(drop=True)\n",
    "y_train = pd.Series(y_train).reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Step 2: Define the reinforcement learning environment\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        return self.X.iloc[self.current_index]  \n",
    "    \n",
    "    def step(self, action):\n",
    "        correct_action = self.y.iloc[self.current_index]  \n",
    "        correct_action_value = 1 if correct_action == \"Portmap\" else 0\n",
    "        reward = 1 if action == correct_action_value else -1\n",
    "        self.current_index += 1\n",
    "        done = self.current_index >= len(self.X)  \n",
    "        next_state = self.X.iloc[self.current_index] if not done else None  \n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "# Step 3: Implementation of the agent using Q-learning with ε-greedy\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_rate=0.95, exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay_rate=0.99):\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        self.n_actions = n_actions\n",
    "        self.state_to_index_mapping = {}  \n",
    "        self.next_state_index = 0 \n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        state_index = self.state_to_index(state)  \n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_index = self.state_to_index(state)\n",
    "        next_state_index = self.state_to_index(next_state) if next_state is not None else None\n",
    "        next_max = np.max(self.q_table[next_state_index]) if next_state_index is not None else 0\n",
    "        self.q_table[state_index, action] += self.learning_rate * (reward + self.discount_rate * next_max - self.q_table[state_index, action])\n",
    "    \n",
    "    def update_exploration_rate(self):\n",
    "        self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay_rate)\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        state_key = str(state)\n",
    "        if state_key not in self.state_to_index_mapping:\n",
    "            self.state_to_index_mapping[state_key] = self.next_state_index\n",
    "            self.next_state_index += 1\n",
    "        return self.state_to_index_mapping[state_key]\n",
    "\n",
    "# Step 4: Simulation of the training\n",
    "\n",
    "def train(agent, environment, episodes=10):\n",
    "    total_rewards = []  \n",
    "    correct_actions_count = []  \n",
    "    exploration_rate_progress = []  \n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        correct_action_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = environment.step(action)\n",
    "            if reward > 0:\n",
    "                correct_action_count += 1\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        agent.update_exploration_rate()\n",
    "        total_rewards.append(total_reward)\n",
    "        correct_actions_count.append(correct_action_count)\n",
    "        exploration_rate_progress.append(agent.exploration_rate)\n",
    "\n",
    "        print(f\"Episode {episode+1}: Total Reward = {total_reward}, Correct Actions = {correct_action_count}, Exploration Rate = {agent.exploration_rate}\")\n",
    "\n",
    "    return total_rewards, correct_actions_count, exploration_rate_progress\n",
    "\n",
    "# Initialize the environment and the agent\n",
    "env = Environment(X_train, y_train)\n",
    "agent = Agent(len(X_train), 2)  \n",
    "\n",
    "# Start the training\n",
    "total_rewards, correct_actions_count, exploration_rate_progress = train(agent, env)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3405a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
